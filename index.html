<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!--
    File: index.html
    Jason Downing
    Contact: jason@downing.io
    MIT Licensed - see http://opensource.org/licenses/MIT for details.
    Anyone may freely use this code. Just don't sue me if it breaks stuff.
    Created: September 3rd, 2015.
    Last Updated: November 13th, 2021.

    Jason Downing's personal website. This site has information about him,
    details about his education, skills, and links to his GitHub and LinkedIn
    Profiles. This site is hosted on GitHub Pages.
  -->

  <!-- Custom Title Bar -->
  <title>Krishnanunni C G</title>

	<!-- 11/13/2021: adding latest font awesome
	--->
  <script src="https://kit.fontawesome.com/a6c9a7a3a0.js" crossorigin="anonymous"></script>
	<!-- Google fonts-->
	<link href="https://fonts.googleapis.com/css?family=Merriweather+Sans:400,700" rel="stylesheet" />
	<link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic" rel="stylesheet" type="text/css" />
	<!-- Third party plugin CSS-->
	<link href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css" rel="stylesheet" />

  <!-- Theme CSS -->
  <link href="css/creative.css" rel="stylesheet">

  <!-- Index CSS -->
  <link href="css/index.css" rel="stylesheet">

  <!-- Custom favicon -->
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon">

  <!-- Google Analytics Tracking Tag. Track how people access my site and such.
       (assuming they don't block ads or whatever from Google) -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-68657454-1', 'auto');
    ga('send', 'pageview');
  </script>


  
        
  </section>

  <!-- Work Experience Section. -->
  <section class="page-section">
    <div class="container-fluid p-0">
      <div class="row no-gutters">
        <div class="col-lg-12 text-center">
          <h2 class="section-heading" id="work">Machine Learning Research</h2>
          <i class="fas fa-4x fa-puzzle-piece text-primary sr-icons"></i>
          <hr class="divider my-4" />
  
				   <!-- Some fun stuff, leave the boring code for last. -->
           <section class="page-section bg-primary">
            <div class="container-fluid">
              <div class="row images-vcenter">
              
               
          
          
              </div>
              <div class="col-lg-12 col-sm-12 text-center">
                <h3 class="mt-4 mb-4">Layerwise Sparsifying Training and Sequential Learning Strategy for Neural Architecture Adaptation</h3>
                <p class="text-faded"> It has been observed that deep neural networks (DNN) create increasingly
                  simpler but more useful  representations  of the learning problem layer by layer. Furthermore, empirical
                  evidence supports the paradigm that depth of a network is of crucial
                  importance. Such large networks, however, yield
                  computationally complex optimization problems. Furthermore, despite
                  such successes, the mechanisms behind deep learning remain a mystery
                  and a trial-and-error approach (Architecture search) is often employed to retrieve the best
                  neural network.  Thus, there is a need for
                  adaptive principles to guide the architecture design of a neural network.
                  
                  <p align="center">
                  <img src="schematic.png">
                  <figcaption>Figure 1: Schematic of layerwise training Algorithm.</figcaption>
                  </p>
                  
                  
                  
                  One of the most promising directions is perhaps the layerwise training of neural
                  networks (Algorithm I).  In this project, the layers of a resnet architecture is trained one at a time, and once they are trained, the input data is mapped forward through the layer to create a new learning problem.  This  is then
                  followed by a sparse training
                  of the enriched NN with L1-regularization only on the weights
                  and biases of the newly added hidden layer. In order to promote learning in subsequent layers and to allow for effective information transfer, we use a manifold regularization term which is based on the similarity in the input data set. Further, we also incorporate a physics informed regularizer for each layer in an attempt to create interpretable hidden layers in a deep neural network. However, the layer-wise training strategy suffers from the training saturation problem where the loss does not decrease after adding a few layers. In order to overcome this issue, a sequential learning strategy (Algorithm 2) is employed
                  where a sequence of small networks is trained to learn the residual produced by Algorithm 1.  
                  
                  
                  ### Significant Results
                  
                  We have additionally demonstrated the approach on a wide variety of problems in scientific machine learning (goverened by PDE's) such as:
                  
                  * Image classification problem.
                  * Physics informed adaptive neural network (PIANN): A framework for adaptively solving PDE's.
                  * Physics reinforced adaptive neural network (PRANN): Combining sparse noisy measurement data with incomplete/approximate physics.
                  * Adaptive learning for inverse problems.
                  
                  Further, we have also compared our proposed approach with other state of the art layerwise training methods. Figure 2 shows such a comparison for the clasification task and also shows a layerwise training curve for a regression task.
                  
                  
                  
                  <p align="center">
                  <img src="summary.png">
                  <figcaption>Figure 2: a) Layerwise training curve on a regression task where the ridge indicates the point where we add a new layer; b) Summary of results for MNIST classification task.</figcaption>
                  </p>
                  
                  Figure 3 below shows the results (evolution of solution with layer addition) for PIANN for learing a PDE with complex geometry.   
                  
                  
                  <p align="center">
                  <img src="PIANN.png">
                  <figcaption>Figure 3: Physics informed adaptive neural network for progressively learning the Poisson's equation with a slit in the domain.</figcaption>
                  </p>
                  
                  We have also demonstrated that our proposed approach serves as a natural candidate for recovering stable inverse maps from sparse data. Inverse problems are usually ill-posed and  involves learning the map from low-dimensional space (observation space) to a high dimensional space (parameter space). By exploring the relationship between manifold regularization and stability, we could enforce stability (well-posedness) while adding new layers. Figure 4 below shows a comparison between different methods for learning inverse maps.
                   
                  <p align="center">
                  <img src="inverse.png">
                  <figcaption>Figure 4: Predicted parameter field for a particular test observation sample using different methods: a) Solution obtained by equivalent baseline network; b) True solution; c)  Solution
                  obtained by Proposed method.</figcaption>
                  </p>
                      
                   <span class="RlRed"></span></p>
                  
                </div>
          
       </div>
      
  
</body>
</html>
